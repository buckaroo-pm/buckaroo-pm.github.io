<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css"/><meta name="next-head-count" content="3"/><link rel="preload" href="/_next/static/css/a0584ae5515792e72ab5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a0584ae5515792e72ab5.css" data-n-g=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/main-7d1613afc8c8f43cb50f.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-95c2b224bccf352ee870.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.ae3781fe50e43492a499.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.9b549cceddc4472f1953.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-3e4ffec4bbf3a45ef50f.js" as="script"/><link rel="preload" href="/_next/static/chunks/c18d713dde5d8f274a9340ae8a63196292880c5e.7af0c42e7b17c91d2be5.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/packages/%5Bowner%5D/%5Bname%5D-bc25a6cf0982fd349b0a.js" as="script"/></head><body><div id="__next"><div class="site"><div class="navigation"><div class="view row space-between"><div class="links"><a class="buckaroo" href="/">Buckaroo</a><a href="https://github.com/LoopPerfect/buckaroo/wiki" target="_blank">Docs</a><a href="/blog">Blog</a></div><div class="search"><input placeholder="search for a module..." value=""/><button class="fa fa-search fa-large"></button></div><iframe id="star-button" src="https://ghbtns.com/github-btn.html?user=LoopPerfect&amp;repo=buckaroo&amp;type=star&amp;count=true&amp;size=large" frameBorder="0" scrolling="0" width="160px" height="30px"></iframe></div></div><div></div><div class="banner"><div style="display:flex"><div class="package"><img src="/packages/buckaroo-pm/codeplea-genann/logo.png"/></div><div><h1>buckaroo-pm/codeplea-genann</h1><div style="max-width:600px">simple neural network library in ANSI C</div><br/><div><b>1068</b></div></div></div><div style="display:inline-flex;flex-flow:row wrap"><span class="tag" style="border-radius:5px;padding:10px;background:white;color:black;font-size:0.7em;margin:5px">backpropagation</span><span class="tag" style="border-radius:5px;padding:10px;background:white;color:black;font-size:0.7em;margin:5px">genetic-algorithm</span><span class="tag" style="border-radius:5px;padding:10px;background:white;color:black;font-size:0.7em;margin:5px">artificial-neural-networks</span><span class="tag" style="border-radius:5px;padding:10px;background:white;color:black;font-size:0.7em;margin:5px">ann</span><span class="tag" style="border-radius:5px;padding:10px;background:white;color:black;font-size:0.7em;margin:5px">neurons</span><span class="tag" style="border-radius:5px;padding:10px;background:white;color:black;font-size:0.7em;margin:5px">hidden-layers</span><span class="tag" style="border-radius:5px;padding:10px;background:white;color:black;font-size:0.7em;margin:5px">neural-network</span><span class="tag" style="border-radius:5px;padding:10px;background:white;color:black;font-size:0.7em;margin:5px">neural-networks</span><span class="tag" style="border-radius:5px;padding:10px;background:white;color:black;font-size:0.7em;margin:5px">neural</span><span class="tag" style="border-radius:5px;padding:10px;background:white;color:black;font-size:0.7em;margin:5px">ansi</span><span class="tag" style="border-radius:5px;padding:10px;background:white;color:black;font-size:0.7em;margin:5px">c</span><span class="tag" style="border-radius:5px;padding:10px;background:white;color:black;font-size:0.7em;margin:5px">tiny</span></div></div><div style="display:flex;width:100%;justify-content:center;margin-top:-2.7em"><div class="tab-control" style="padding:0.5em;border-radius:0.2em;font-size:1.5em;background:white;color:black;border:solid 1px #fff">ReadMe</div><div class="tab-control" style="padding:0.5em;border-radius:0.2em;font-size:1.5em;background:none;color:white;border:solid 1px #fff">Buck</div><div class="tab-control" style="padding:0.5em;border-radius:0.2em;font-size:1.5em;background:none;color:white;border:solid 1px #fff">Manifest</div><div class="tab-control" style="padding:0.5em;border-radius:0.2em;font-size:1.5em;background:none;color:white;border:solid 1px #fff">Dependencies</div></div><div class="banner white"><div class="view column"><h2>ReadMe</h2><p><a href="https://travis-ci.org/codeplea/genann"><img src="https://travis-ci.org/codeplea/genann.svg?branch=master" alt="Build Status"/></a></p>&lt;img alt=&quot;Genann logo&quot; src=&quot;https://codeplea.com/public/content/genann_logo.png&quot; align=&quot;right&quot; /&gt;<h1>Genann</h1><p>Genann is a minimal, well-tested library for training and using feedforward
artificial neural networks (ANN) in C. Its primary focus is on being simple,
fast, reliable, and hackable. It achieves this by providing only the necessary
functions and little extra.</p><h2>Features</h2><ul><li><strong>ANSI C with no dependencies</strong>.</li><li>Contained in a single source code and header file.</li><li>Simple.</li><li>Fast and thread-safe.</li><li>Easily extendible.</li><li>Implements backpropagation training.</li><li><em>Compatible with alternative training methods</em> (classic optimization, genetic algorithms, etc)</li><li>Includes examples and test suite.</li><li>Released under the zlib license - free for nearly any use.</li></ul><h2>Building</h2><p>Genann is self-contained in two files: <code>genann.c</code> and <code>genann.h</code>. To use Genann, simply add those two files to your project.</p><h2>Example Code</h2><p>Four example programs are included with the source code.</p><ul><li><a href="./example1.c"><code>example1.c</code></a> - Trains an ANN on the XOR function using backpropagation.</li><li><a href="./example2.c"><code>example2.c</code></a> - Trains an ANN on the XOR function using random search.</li><li><a href="./example3.c"><code>example3.c</code></a> - Loads and runs an ANN from a file.</li><li><a href="./example4.c"><code>example4.c</code></a> - Trains an ANN on the <a href="https://archive.ics.uci.edu/ml/datasets/Iris">IRIS data-set</a> using backpropagation.</li></ul><h2>Quick Example</h2><p>We create an ANN taking 2 inputs, having 1 layer of 3 hidden neurons, and
providing 2 outputs. It has the following structure:</p><p><img src="./doc/e1.png" alt="NN Example Structure"/></p><p>We then train it on a set of labeled data using backpropagation and ask it to
predict on a test data point:</p><pre style="color:black;background:#f5f2f0;text-shadow:0 1px white;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto"><code class="language-C" style="color:black;background:none;text-shadow:0 1px white;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>#include &quot;genann.h&quot;
</span>
<!-- -->/* Not shown, loading your training and test data. */
<!-- -->double **training_data_input, **training_data_output, **test_data_input;
<!-- -->
<!-- -->/* New network with 2 inputs,
<!-- --> * 1 hidden layer of 3 neurons each,
<!-- --> * and 2 outputs. */
<!-- -->genann *ann = genann_init(2, 1, 3, 2);
<!-- -->
<!-- -->/* Learn on the training set. */
<!-- -->for (i = 0; i &lt; 300; ++i) {
<!-- -->    for (j = 0; j &lt; 100; ++j)
<!-- -->        genann_train(ann, training_data_input[j], training_data_output[j], 0.1);
<!-- -->}
<!-- -->
<!-- -->/* Run the network and see what it predicts. */
<!-- -->double const *prediction = genann_run(ann, test_data_input[0]);
<!-- -->printf(&quot;Output for the first test data point is: %f, %f\n&quot;, prediction[0], prediction[1]);
<!-- -->
<!-- -->genann_free(ann);
</code></pre><p>This example is to show API usage, it is not showing good machine learning
techniques. In a real application you would likely want to learn on the test
data in a random order. You would also want to monitor the learning to prevent
over-fitting.</p><h2>Usage</h2><h3>Creating and Freeing ANNs</h3><pre style="color:black;background:#f5f2f0;text-shadow:0 1px white;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto"><code class="language-C" style="color:black;background:none;text-shadow:0 1px white;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>genann *genann_init(int inputs, int hidden_layers, int hidden, int outputs);
</span>genann *genann_copy(genann const *ann);
<!-- -->void genann_free(genann *ann);
</code></pre><p>Creating a new ANN is done with the <code>genann_init()</code> function. Its arguments
are the number of inputs, the number of hidden layers, the number of neurons in
each hidden layer, and the number of outputs. It returns a <code>genann</code> struct pointer.</p><p>Calling <code>genann_copy()</code> will create a deep-copy of an existing <code>genann</code> struct.</p><p>Call <code>genann_free()</code> when you&#x27;re finished with an ANN returned by <code>genann_init()</code>.</p><h3>Training ANNs</h3><pre style="color:black;background:#f5f2f0;text-shadow:0 1px white;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto"><code class="language-C" style="color:black;background:none;text-shadow:0 1px white;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>void genann_train(genann const *ann, double const *inputs,
</span>        double const *desired_outputs, double learning_rate);
</code></pre><p><code>genann_train()</code> will preform one update using standard backpropogation. It
should be called by passing in an array of inputs, an array of expected outputs,
and a learning rate. See <em>example1.c</em> for an example of learning with
backpropogation.</p><p>A primary design goal of Genann was to store all the network weights in one
contigious block of memory. This makes it easy and efficient to train the
network weights using direct-search numeric optimizion algorthims,
such as <a href="https://en.wikipedia.org/wiki/Hill_climbing">Hill Climbing</a>,
<a href="https://en.wikipedia.org/wiki/Genetic_algorithm">the Genetic Algorithm</a>, <a href="https://en.wikipedia.org/wiki/Simulated_annealing">Simulated
Annealing</a>, etc.
These methods can be used by searching on the ANN&#x27;s weights directly.
Every <code>genann</code> struct contains the members <code>int total_weights;</code> and
<code>double *weight;</code>.  <code>*weight</code> points to an array of <code>total_weights</code>
size which contains all weights used by the ANN. See <em>example2.c</em> for
an example of training using random hill climbing search.</p><h3>Saving and Loading ANNs</h3><pre style="color:black;background:#f5f2f0;text-shadow:0 1px white;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto"><code class="language-C" style="color:black;background:none;text-shadow:0 1px white;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>genann *genann_read(FILE *in);
</span>void genann_write(genann const *ann, FILE *out);
</code></pre><p>Genann provides the <code>genann_read()</code> and <code>genann_write()</code> functions for loading or saving an ANN in a text-based format.</p><h3>Evaluating</h3><pre style="color:black;background:#f5f2f0;text-shadow:0 1px white;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;padding:1em;margin:.5em 0;overflow:auto"><code class="language-C" style="color:black;background:none;text-shadow:0 1px white;font-family:Consolas, Monaco, &#x27;Andale Mono&#x27;, &#x27;Ubuntu Mono&#x27;, monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none"><span>double const *genann_run(genann const *ann, double const *inputs);</span></code></pre><p>Call <code>genann_run()</code> on a trained ANN to run a feed-forward pass on a given set of inputs. <code>genann_run()</code>
will provide a pointer to the array of predicted outputs (of <code>ann-&gt;outputs</code> length).</p><h2>Hints</h2><ul><li>All functions start with <code>genann_</code>.</li><li>The code is simple. Dig in and change things.</li></ul><h2>Extra Resources</h2><p>The <a href="http://www.faqs.org/faqs/ai-faq/neural-nets/part1/">comp.ai.neural-nets
FAQ</a> is an excellent
resource for an introduction to artificial neural networks.</p><p>If you need an even smaller neural network library, check out the excellent single-hidden-layer library <a href="https://github.com/glouw/tinn">tinn</a>.</p><p>If you&#x27;re looking for a heavier, more opinionated neural network library in C,
I recommend the <a href="http://leenissen.dk/fann/wp/">FANN library</a>. Another
good library is Peter van Rossum&#x27;s <a href="http://lwneuralnet.sourceforge.net/">Lightweight Neural
Network</a>, which despite its name, is
heavier and has more features than Genann.</p></div></div><div class="banner footer"><div class="view column"><div class="actions"><div class="links"><a href="/">Buckaroo</a><a href="https://github.com/LoopPerfect/buckaroo/wiki" target="_blank">Docs</a><a href="https://github.com/LoopPerfect/buckaroo" target="_blank">GitHub</a></div><div class="signup"><h1 class="action">Sign up for our Newsletter</h1><div class="info">Get the latest updates about new packages and Buckaroo releases.</div><button>Sign Up</button></div></div><div class="powered"><img src="/LoopPerfectInvertSmall.png"/><span>Powered by LoopPerfect</span></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"packageName":"buckaroo-pm/codeplea-genann","name":"codeplea/genann","licence":"Zlib","description":"simple neural network library in ANSI C","readme":"[![Build Status](https://travis-ci.org/codeplea/genann.svg?branch=master)](https://travis-ci.org/codeplea/genann)\n\n\u003cimg alt=\"Genann logo\" src=\"https://codeplea.com/public/content/genann_logo.png\" align=\"right\" /\u003e\n\n# Genann\n\nGenann is a minimal, well-tested library for training and using feedforward\nartificial neural networks (ANN) in C. Its primary focus is on being simple,\nfast, reliable, and hackable. It achieves this by providing only the necessary\nfunctions and little extra.\n\n## Features\n\n- **ANSI C with no dependencies**.\n- Contained in a single source code and header file.\n- Simple.\n- Fast and thread-safe.\n- Easily extendible.\n- Implements backpropagation training.\n- *Compatible with alternative training methods* (classic optimization, genetic algorithms, etc)\n- Includes examples and test suite.\n- Released under the zlib license - free for nearly any use.\n\n## Building\n\nGenann is self-contained in two files: `genann.c` and `genann.h`. To use Genann, simply add those two files to your project.\n\n## Example Code\n\nFour example programs are included with the source code.\n\n- [`example1.c`](./example1.c) - Trains an ANN on the XOR function using backpropagation.\n- [`example2.c`](./example2.c) - Trains an ANN on the XOR function using random search.\n- [`example3.c`](./example3.c) - Loads and runs an ANN from a file.\n- [`example4.c`](./example4.c) - Trains an ANN on the [IRIS data-set](https://archive.ics.uci.edu/ml/datasets/Iris) using backpropagation.\n\n## Quick Example\n\nWe create an ANN taking 2 inputs, having 1 layer of 3 hidden neurons, and\nproviding 2 outputs. It has the following structure:\n\n![NN Example Structure](./doc/e1.png)\n\nWe then train it on a set of labeled data using backpropagation and ask it to\npredict on a test data point:\n\n```C\n#include \"genann.h\"\n\n/* Not shown, loading your training and test data. */\ndouble **training_data_input, **training_data_output, **test_data_input;\n\n/* New network with 2 inputs,\n * 1 hidden layer of 3 neurons each,\n * and 2 outputs. */\ngenann *ann = genann_init(2, 1, 3, 2);\n\n/* Learn on the training set. */\nfor (i = 0; i \u003c 300; ++i) {\n    for (j = 0; j \u003c 100; ++j)\n        genann_train(ann, training_data_input[j], training_data_output[j], 0.1);\n}\n\n/* Run the network and see what it predicts. */\ndouble const *prediction = genann_run(ann, test_data_input[0]);\nprintf(\"Output for the first test data point is: %f, %f\\n\", prediction[0], prediction[1]);\n\ngenann_free(ann);\n```\n\nThis example is to show API usage, it is not showing good machine learning\ntechniques. In a real application you would likely want to learn on the test\ndata in a random order. You would also want to monitor the learning to prevent\nover-fitting.\n\n\n## Usage\n\n### Creating and Freeing ANNs\n```C\ngenann *genann_init(int inputs, int hidden_layers, int hidden, int outputs);\ngenann *genann_copy(genann const *ann);\nvoid genann_free(genann *ann);\n```\n\nCreating a new ANN is done with the `genann_init()` function. Its arguments\nare the number of inputs, the number of hidden layers, the number of neurons in\neach hidden layer, and the number of outputs. It returns a `genann` struct pointer.\n\nCalling `genann_copy()` will create a deep-copy of an existing `genann` struct.\n\nCall `genann_free()` when you're finished with an ANN returned by `genann_init()`.\n\n\n### Training ANNs\n```C\nvoid genann_train(genann const *ann, double const *inputs,\n        double const *desired_outputs, double learning_rate);\n```\n\n`genann_train()` will preform one update using standard backpropogation. It\nshould be called by passing in an array of inputs, an array of expected outputs,\nand a learning rate. See *example1.c* for an example of learning with\nbackpropogation.\n\nA primary design goal of Genann was to store all the network weights in one\ncontigious block of memory. This makes it easy and efficient to train the\nnetwork weights using direct-search numeric optimizion algorthims,\nsuch as [Hill Climbing](https://en.wikipedia.org/wiki/Hill_climbing),\n[the Genetic Algorithm](https://en.wikipedia.org/wiki/Genetic_algorithm), [Simulated\nAnnealing](https://en.wikipedia.org/wiki/Simulated_annealing), etc.\nThese methods can be used by searching on the ANN's weights directly.\nEvery `genann` struct contains the members `int total_weights;` and\n`double *weight;`.  `*weight` points to an array of `total_weights`\nsize which contains all weights used by the ANN. See *example2.c* for\nan example of training using random hill climbing search.\n\n### Saving and Loading ANNs\n\n```C\ngenann *genann_read(FILE *in);\nvoid genann_write(genann const *ann, FILE *out);\n```\n \nGenann provides the `genann_read()` and `genann_write()` functions for loading or saving an ANN in a text-based format.\n\n### Evaluating\n\n```C\ndouble const *genann_run(genann const *ann, double const *inputs);\n```\n\nCall `genann_run()` on a trained ANN to run a feed-forward pass on a given set of inputs. `genann_run()`\nwill provide a pointer to the array of predicted outputs (of `ann-\u003eoutputs` length).\n\n\n## Hints\n\n- All functions start with `genann_`.\n- The code is simple. Dig in and change things.\n\n## Extra Resources\n\nThe [comp.ai.neural-nets\nFAQ](http://www.faqs.org/faqs/ai-faq/neural-nets/part1/) is an excellent\nresource for an introduction to artificial neural networks.\n\nIf you need an even smaller neural network library, check out the excellent single-hidden-layer library [tinn](https://github.com/glouw/tinn).\n\nIf you're looking for a heavier, more opinionated neural network library in C,\nI recommend the [FANN library](http://leenissen.dk/fann/wp/). Another\ngood library is Peter van Rossum's [Lightweight Neural\nNetwork](http://lwneuralnet.sourceforge.net/), which despite its name, is\nheavier and has more features than Genann.\n","versions":[{"ref":"master","manifest":"targets = [ \"//:genann\" ]\n","lockFile":"","buck":"cxx_library(\n  name = 'genann',\n  header_namespace = '',\n  srcs = ['genann.c'],\n  exported_headers = [\n    'genann.h'\n  ],\n  visibility = [\n    'PUBLIC',\n  ],\n)\n\ncxx_binary(\n  name = 'test',\n  srcs = ['test.c'],\n  deps = [':genann']\n)\n","bazel":"","deps":[],"lock":[]}],"updated":"2019-01-23T14:33:05Z","updatedUpstream":"2021-01-11T17:15:30Z","contributors":[{"login":"amboar","avatarUrl":"https://avatars0.githubusercontent.com/u/526481?v=4"},{"login":"crclark96","avatarUrl":"https://avatars1.githubusercontent.com/u/9776239?u=a2d3a470542cc50b35bf270bf4d56bd7bcfbebcd\u0026v=4"},{"login":"Dickby","avatarUrl":"https://avatars2.githubusercontent.com/u/10100765?v=4"},{"login":"codeplea","avatarUrl":"https://avatars1.githubusercontent.com/u/14815717?v=4"},{"login":"jflopezfernandez","avatarUrl":"https://avatars1.githubusercontent.com/u/17623083?u=816f162d12c8f0c0535849502b457b2459ebe6f4\u0026v=4"},{"login":"timgates42","avatarUrl":"https://avatars1.githubusercontent.com/u/47873678?u=5979ac643b54da99a384068c6aa23fa56296b87e\u0026v=4"}],"fundingLinks":[],"contactLinks":[],"stars":1068,"forks":138,"topics":["backpropagation","genetic-algorithm","artificial-neural-networks","ann","neurons","hidden-layers","neural-network","neural-networks","neural","ansi","c","tiny"]}},"__N_SSG":true},"page":"/packages/[owner]/[name]","query":{"owner":"buckaroo-pm","name":"codeplea-genann"},"buildId":"XEnVUsOG7VkTdg59Wro6f","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-617a3a2c770be2525c7f.js"></script><script src="/_next/static/chunks/main-7d1613afc8c8f43cb50f.js" async=""></script><script src="/_next/static/chunks/webpack-95c2b224bccf352ee870.js" async=""></script><script src="/_next/static/chunks/framework.ae3781fe50e43492a499.js" async=""></script><script src="/_next/static/chunks/commons.9b549cceddc4472f1953.js" async=""></script><script src="/_next/static/chunks/pages/_app-3e4ffec4bbf3a45ef50f.js" async=""></script><script src="/_next/static/chunks/c18d713dde5d8f274a9340ae8a63196292880c5e.7af0c42e7b17c91d2be5.js" async=""></script><script src="/_next/static/chunks/pages/packages/%5Bowner%5D/%5Bname%5D-bc25a6cf0982fd349b0a.js" async=""></script><script src="/_next/static/XEnVUsOG7VkTdg59Wro6f/_buildManifest.js" async=""></script><script src="/_next/static/XEnVUsOG7VkTdg59Wro6f/_ssgManifest.js" async=""></script></body></html>